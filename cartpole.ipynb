{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bb50b33",
   "metadata": {},
   "source": [
    "# Laboratorio 7 CartPole\n",
    "## Deep Learning\n",
    "\n",
    "- Fabiola Contreras, 22787\n",
    "- Diego Duarte, 22075\n",
    "- José Marchena, 22398\n",
    "- Sofía Velásquez, 22049\n",
    "- María José Villafuerte, 22129"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d24211e",
   "metadata": {},
   "source": [
    "### Preparar ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d64b3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7a6085a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TimeLimit<OrderEnforcing<PassiveEnvChecker<CartPoleEnv<CartPole-v1>>>>>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3b1f49",
   "metadata": {},
   "source": [
    "### Definición de Redes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54198f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, obs_size, n_actions):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_actions)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9cad439e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.3.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"C:\\Users\\diego\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"C:\\Users\\diego\\AppData\\Roaming\\Python\\Python311\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"C:\\Users\\diego\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"C:\\Users\\diego\\AppData\\Roaming\\Python\\Python311\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Users\\diego\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 607, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\diego\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 1922, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\diego\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"C:\\Users\\diego\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"C:\\Users\\diego\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"C:\\Users\\diego\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"C:\\Users\\diego\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"C:\\Users\\diego\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"C:\\Users\\diego\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"C:\\Users\\diego\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"C:\\Users\\diego\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"C:\\Users\\diego\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"C:\\Users\\diego\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"C:\\Users\\diego\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"C:\\Users\\diego\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"C:\\Users\\diego\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\diego\\AppData\\Local\\Temp\\ipykernel_28780\\920487331.py\", line 5, in <module>\n",
      "    online_net = QNetwork(obs_size, n_actions)\n",
      "  File \"C:\\Users\\diego\\AppData\\Local\\Temp\\ipykernel_28780\\3347388919.py\", line 5, in __init__\n",
      "    nn.Linear(obs_size, 128),\n",
      "  File \"c:\\Users\\diego\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\linear.py\", line 96, in __init__\n",
      "    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))\n",
      "c:\\Users\\diego\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:96: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ..\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "QNetwork(\n",
       "  (net): Sequential(\n",
       "    (0): Linear(in_features=4, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=128, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_size = env.observation_space.shape[0]   # 4 observaciones\n",
    "n_actions = env.action_space.n              # 2 acciones\n",
    "\n",
    "# Red en línea\n",
    "online_net = QNetwork(obs_size, n_actions)\n",
    "\n",
    "# Red de destino (clon inicial)\n",
    "target_net = QNetwork(obs_size, n_actions)\n",
    "target_net.load_state_dict(online_net.state_dict())  # mismos pesos al inicio\n",
    "target_net.eval()  # no necesita gradientes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4326c22e",
   "metadata": {},
   "source": [
    "### Establecer hiperparámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "662a85d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import deque\n",
    "import torch.optim as optim\n",
    "\n",
    "# Dispositivo\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "online_net.to(device)\n",
    "target_net.to(device)\n",
    "\n",
    "# Hiperparámetros principales\n",
    "GAMMA = 0.99                 # factor de descuento\n",
    "LR = 1e-3                    # tasa de aprendizaje\n",
    "BATCH_SIZE = 64              # tamaño de minibatch\n",
    "REPLAY_CAPACITY = 50_000     # capacidad del buffer\n",
    "TARGET_UPDATE_FREQ = 1000    # cada cuántos pasos copiar (hard update) a la red objetivo\n",
    "TRAIN_START_SIZE = 1000      # mínimo de transiciones antes de entrenar\n",
    "MAX_EPISODES = 500           # por si lo usas luego en el bucle de entrenamiento\n",
    "MAX_STEPS_PER_EP = 500\n",
    "\n",
    "# Exploración ε-greedy\n",
    "EPS_START = 1.0\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY_STEPS = 50_000     # pasos para decaer desde 1.0 -> 0.05 (lineal)\n",
    "\n",
    "# Optimizador y pérdida\n",
    "optimizer = optim.Adam(online_net.parameters(), lr=LR)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Programador lineal de epsilon (por pasos globales)\n",
    "global_step = 0\n",
    "def get_epsilon(step: int) -> float:\n",
    "    # decaimiento lineal\n",
    "    frac = min(1.0, step / EPS_DECAY_STEPS)\n",
    "    return EPS_START + (EPS_END - EPS_START) * frac"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7e2ec5",
   "metadata": {},
   "source": [
    "### Selección de acciones épsilon-greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9c82e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state_np, epsilon: float):\n",
    "    \"\"\"\n",
    "    state_np: np.ndarray con forma (obs_size,)\n",
    "    epsilon : valor actual de exploración\n",
    "    Retorna: acción (int)\n",
    "    \"\"\"\n",
    "    if random.random() < epsilon:\n",
    "        # Explora\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        # Explotación con la red online\n",
    "        state_t = torch.as_tensor(state_np, dtype=torch.float32, device=device).unsqueeze(0)  # [1, obs_size]\n",
    "        with torch.no_grad():\n",
    "            q_values = online_net(state_t)  # [1, n_actions]\n",
    "            action = int(torch.argmax(q_values, dim=1).item())\n",
    "        return action\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adf6a15",
   "metadata": {},
   "source": [
    "### Experience replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5886310f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replay Buffer + paso de optimización (DQL con red objetivo) muy de girly pop\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity: int):\n",
    "        self.buf = deque(maxlen=capacity)\n",
    "    def push(self, s, a, r, s_next, done):\n",
    "        # Guardamos como tuplas\n",
    "        self.buf.append((s, a, r, s_next, done))\n",
    "    def __len__(self):\n",
    "        return len(self.buf)\n",
    "    def sample(self, batch_size: int):\n",
    "        batch = random.sample(self.buf, batch_size)\n",
    "        s, a, r, s_next, d = zip(*batch)\n",
    "\n",
    "        s      = torch.as_tensor(np.vstack(s), dtype=torch.float32, device=device)      # [B, obs]\n",
    "        a      = torch.as_tensor(a, dtype=torch.int64, device=device).unsqueeze(1)      # [B, 1]\n",
    "        r      = torch.as_tensor(r, dtype=torch.float32, device=device).unsqueeze(1)    # [B, 1]\n",
    "        s_next = torch.as_tensor(np.vstack(s_next), dtype=torch.float32, device=device) # [B, obs]\n",
    "        d      = torch.as_tensor(d, dtype=torch.float32, device=device).unsqueeze(1)    # [B, 1]\n",
    "\n",
    "        return s, a, r, s_next, d\n",
    "\n",
    "\n",
    "replay = ReplayBuffer(REPLAY_CAPACITY)\n",
    "\n",
    "def hard_update_target():\n",
    "    \"\"\"Copia dura de pesos desde online_net hacia target_net.\"\"\"\n",
    "    target_net.load_state_dict(online_net.state_dict())\n",
    "\n",
    "def optimize_model():\n",
    "    \"\"\"Un paso de optimización DQL usando muestras del replay + red objetivo.\"\"\"\n",
    "    if len(replay) < max(BATCH_SIZE, TRAIN_START_SIZE):\n",
    "        return None  # aún no hay suficientes muestras\n",
    "\n",
    "    # Muestra minibatch\n",
    "    states, actions, rewards, next_states, dones = replay.sample(BATCH_SIZE)\n",
    "\n",
    "    # Q_online(s, a): tomamos sólo el Q de la acción ejecutada\n",
    "    q_pred_all = online_net(states)                       # [B, n_actions]\n",
    "    q_pred = q_pred_all.gather(1, actions)                # [B, 1]\n",
    "\n",
    "    # Objetivo con red objetivo: y = r + gamma * (1 - done) * max_a' Q_target(s', a')\n",
    "    with torch.no_grad():\n",
    "        q_next_all = target_net(next_states)              # [B, n_actions]\n",
    "        q_next_max = q_next_all.max(dim=1, keepdim=True).values  # [B, 1]\n",
    "        q_target = rewards + (1.0 - dones) * GAMMA * q_next_max  # [B, 1]\n",
    "\n",
    "    # Pérdida MSE\n",
    "    loss = criterion(q_pred, q_target)\n",
    "\n",
    "    # Backprop\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # por si nos sirve, este es un clipping de gradiente para estabilidad, si no lo podemos borrar, x no importa\n",
    "    torch.nn.utils.clip_grad_norm_(online_net.parameters(), max_norm=10.0)\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f3dfe5",
   "metadata": {},
   "source": [
    "### Training Cycle y Visualizar Entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d08d48bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 10/500 | Recompensa media ult.10: 24.00 | ε=0.724\n",
      "Ep 20/500 | Recompensa media ult.10: 23.60 | ε=0.719\n",
      "Ep 30/500 | Recompensa media ult.10: 27.80 | ε=0.714\n",
      "Ep 40/500 | Recompensa media ult.10: 34.30 | ε=0.708\n",
      "Ep 50/500 | Recompensa media ult.10: 34.10 | ε=0.701\n",
      "Ep 60/500 | Recompensa media ult.10: 30.70 | ε=0.695\n",
      "Ep 70/500 | Recompensa media ult.10: 31.70 | ε=0.689\n",
      "Ep 80/500 | Recompensa media ult.10: 31.00 | ε=0.683\n",
      "Ep 90/500 | Recompensa media ult.10: 20.70 | ε=0.679\n",
      "Ep 100/500 | Recompensa media ult.10: 29.90 | ε=0.674\n",
      "Ep 110/500 | Recompensa media ult.10: 30.70 | ε=0.668\n",
      "Ep 120/500 | Recompensa media ult.10: 22.10 | ε=0.664\n",
      "Ep 130/500 | Recompensa media ult.10: 46.10 | ε=0.655\n",
      "Ep 140/500 | Recompensa media ult.10: 23.40 | ε=0.651\n",
      "Ep 150/500 | Recompensa media ult.10: 45.70 | ε=0.642\n",
      "Ep 160/500 | Recompensa media ult.10: 45.80 | ε=0.633\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 37\u001b[0m\n\u001b[0;32m     28\u001b[0m replay\u001b[38;5;241m.\u001b[39mpush(\n\u001b[0;32m     29\u001b[0m     np\u001b[38;5;241m.\u001b[39marray(state, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32), \n\u001b[0;32m     30\u001b[0m     action, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     33\u001b[0m     done\n\u001b[0;32m     34\u001b[0m )\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Optimizar modelo con un batch de replay\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43moptimize_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     39\u001b[0m     loss_history\u001b[38;5;241m.\u001b[39mappend(loss)\n",
      "Cell \u001b[1;32mIn[10], line 38\u001b[0m, in \u001b[0;36moptimize_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m     35\u001b[0m states, actions, rewards, next_states, dones \u001b[38;5;241m=\u001b[39m replay\u001b[38;5;241m.\u001b[39msample(BATCH_SIZE)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Q_online(s, a): tomamos sólo el Q de la acción ejecutada\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m q_pred_all \u001b[38;5;241m=\u001b[39m \u001b[43monline_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m)\u001b[49m                       \u001b[38;5;66;03m# [B, n_actions]\u001b[39;00m\n\u001b[0;32m     39\u001b[0m q_pred \u001b[38;5;241m=\u001b[39m q_pred_all\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m1\u001b[39m, actions)                \u001b[38;5;66;03m# [B, 1]\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Objetivo con red objetivo: y = r + gamma * (1 - done) * max_a' Q_target(s', a')\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\diego\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[3], line 13\u001b[0m, in \u001b[0;36mQNetwork.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\diego\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\diego\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\diego\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\diego\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:103\u001b[0m, in \u001b[0;36mReLU.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\diego\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\functional.py:1457\u001b[0m, in \u001b[0;36mrelu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m   1455\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m   1456\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1457\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1458\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Crear entorno con renderizado en pantalla\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")  \n",
    "\n",
    "reward_history = []\n",
    "loss_history = []\n",
    "\n",
    "for episode in range(MAX_EPISODES):\n",
    "    state, _ = env.reset()\n",
    "    total_reward = 0\n",
    "\n",
    "    for t in range(MAX_STEPS_PER_EP):\n",
    "        global_step += 1\n",
    "        epsilon = get_epsilon(global_step)\n",
    "\n",
    "        # Renderizar cada 50 episodios\n",
    "        if episode % 50 == 0:\n",
    "            env.render()\n",
    "\n",
    "        # Selección de acción (ε-greedy)\n",
    "        action = select_action(state, epsilon)\n",
    "\n",
    "        # Ejecutar acción en el entorno\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        total_reward += reward\n",
    "\n",
    "        # Guardar transición en el buffer con dtype correcto\n",
    "        replay.push(\n",
    "            np.array(state, dtype=np.float32), \n",
    "            action, \n",
    "            reward, \n",
    "            np.array(next_state, dtype=np.float32), \n",
    "            done\n",
    "        )\n",
    "\n",
    "        # Optimizar modelo con un batch de replay\n",
    "        loss = optimize_model()\n",
    "        if loss is not None:\n",
    "            loss_history.append(loss)\n",
    "\n",
    "        # Actualizar red objetivo cada cierto número de pasos\n",
    "        if global_step % TARGET_UPDATE_FREQ == 0:\n",
    "            hard_update_target()\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    reward_history.append(total_reward)\n",
    "\n",
    "    # Log de progreso cada 10 episodios\n",
    "    if (episode + 1) % 10 == 0:\n",
    "        avg_r = np.mean(reward_history[-10:])\n",
    "        print(f\"Ep {episode+1}/{MAX_EPISODES} | Recompensa media ult.10: {avg_r:.2f} | ε={epsilon:.3f}\")\n",
    "\n",
    "# Cerrar entorno al final\n",
    "env.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
