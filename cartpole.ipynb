{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bb50b33",
   "metadata": {},
   "source": [
    "# Laboratorio 7 CartPole\n",
    "## Deep Learning\n",
    "\n",
    "- Fabiola Contreras, 22787\n",
    "- Diego Duarte, 22075\n",
    "- José Marchena, 22398\n",
    "- Sofía Velásquez, 22049\n",
    "- María José Villafuerte, 22129"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d24211e",
   "metadata": {},
   "source": [
    "### Preparar ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d64b3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7a6085a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TimeLimit<OrderEnforcing<PassiveEnvChecker<CartPoleEnv<CartPole-v1>>>>>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3b1f49",
   "metadata": {},
   "source": [
    "### Definición de Redes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54198f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, obs_size, n_actions):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_actions)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9cad439e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QNetwork(\n",
       "  (net): Sequential(\n",
       "    (0): Linear(in_features=4, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=128, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_size = env.observation_space.shape[0]   # 4 observaciones\n",
    "n_actions = env.action_space.n              # 2 acciones\n",
    "\n",
    "# Red en línea\n",
    "online_net = QNetwork(obs_size, n_actions)\n",
    "\n",
    "# Red de destino (clon inicial)\n",
    "target_net = QNetwork(obs_size, n_actions)\n",
    "target_net.load_state_dict(online_net.state_dict())  # mismos pesos al inicio\n",
    "target_net.eval()  # no necesita gradientes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4326c22e",
   "metadata": {},
   "source": [
    "### Establecer hiperparámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "662a85d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import deque\n",
    "import torch.optim as optim\n",
    "\n",
    "# Dispositivo\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "online_net.to(device)\n",
    "target_net.to(device)\n",
    "\n",
    "# Hiperparámetros principales\n",
    "GAMMA = 0.99                 # factor de descuento\n",
    "LR = 1e-3                    # tasa de aprendizaje\n",
    "BATCH_SIZE = 64              # tamaño de minibatch\n",
    "REPLAY_CAPACITY = 50_000     # capacidad del buffer\n",
    "TARGET_UPDATE_FREQ = 1000    # cada cuántos pasos copiar (hard update) a la red objetivo\n",
    "TRAIN_START_SIZE = 1000      # mínimo de transiciones antes de entrenar\n",
    "MAX_EPISODES = 500           # por si lo usas luego en el bucle de entrenamiento\n",
    "MAX_STEPS_PER_EP = 500\n",
    "\n",
    "# Exploración ε-greedy\n",
    "EPS_START = 1.0\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY_STEPS = 50_000     # pasos para decaer desde 1.0 -> 0.05 (lineal)\n",
    "\n",
    "# Optimizador y pérdida\n",
    "optimizer = optim.Adam(online_net.parameters(), lr=LR)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Programador lineal de epsilon (por pasos globales)\n",
    "global_step = 0\n",
    "def get_epsilon(step: int) -> float:\n",
    "    # decaimiento lineal\n",
    "    frac = min(1.0, step / EPS_DECAY_STEPS)\n",
    "    return EPS_START + (EPS_END - EPS_START) * frac"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7e2ec5",
   "metadata": {},
   "source": [
    "### Selección de acciones épsilon-greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9c82e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state_np, epsilon: float):\n",
    "    \"\"\"\n",
    "    state_np: np.ndarray con forma (obs_size,)\n",
    "    epsilon : valor actual de exploración\n",
    "    Retorna: acción (int)\n",
    "    \"\"\"\n",
    "    if random.random() < epsilon:\n",
    "        # Explora\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        # Explotación con la red online\n",
    "        state_t = torch.as_tensor(state_np, dtype=torch.float32, device=device).unsqueeze(0)  # [1, obs_size]\n",
    "        with torch.no_grad():\n",
    "            q_values = online_net(state_t)  # [1, n_actions]\n",
    "            action = int(torch.argmax(q_values, dim=1).item())\n",
    "        return action\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adf6a15",
   "metadata": {},
   "source": [
    "### Experience replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5886310f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replay Buffer + paso de optimización (DQL con red objetivo) muy de girly pop\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity: int):\n",
    "        self.buf = deque(maxlen=capacity)\n",
    "    def push(self, s, a, r, s_next, done):\n",
    "        # Guardamos como tuplas\n",
    "        self.buf.append((s, a, r, s_next, done))\n",
    "    def __len__(self):\n",
    "        return len(self.buf)\n",
    "    def sample(self, batch_size: int):\n",
    "        batch = random.sample(self.buf, batch_size)\n",
    "        s, a, r, s_next, d = zip(*batch)\n",
    "        s      = torch.as_tensor(np.array(s, dtype=np.float32), device=device)            # [B, obs]\n",
    "        a      = torch.as_tensor(a, dtype=torch.int64, device=device).unsqueeze(1)        # [B, 1]\n",
    "        r      = torch.as_tensor(r, dtype=torch.float32, device=device).unsqueeze(1)      # [B, 1]\n",
    "        s_next = torch.as_tensor(np.array(s_next, dtype=np.float32), device=device)       # [B, obs]\n",
    "        d      = torch.as_tensor(d, dtype=torch.float32, device=device).unsqueeze(1)      # [B, 1]\n",
    "        return s, a, r, s_next, d\n",
    "\n",
    "replay = ReplayBuffer(REPLAY_CAPACITY)\n",
    "\n",
    "def hard_update_target():\n",
    "    \"\"\"Copia dura de pesos desde online_net hacia target_net.\"\"\"\n",
    "    target_net.load_state_dict(online_net.state_dict())\n",
    "\n",
    "def optimize_model():\n",
    "    \"\"\"Un paso de optimización DQL usando muestras del replay + red objetivo.\"\"\"\n",
    "    if len(replay) < max(BATCH_SIZE, TRAIN_START_SIZE):\n",
    "        return None  # aún no hay suficientes muestras\n",
    "\n",
    "    # Muestra minibatch\n",
    "    states, actions, rewards, next_states, dones = replay.sample(BATCH_SIZE)\n",
    "\n",
    "    # Q_online(s, a): tomamos sólo el Q de la acción ejecutada\n",
    "    q_pred_all = online_net(states)                       # [B, n_actions]\n",
    "    q_pred = q_pred_all.gather(1, actions)                # [B, 1]\n",
    "\n",
    "    # Objetivo con red objetivo: y = r + gamma * (1 - done) * max_a' Q_target(s', a')\n",
    "    with torch.no_grad():\n",
    "        q_next_all = target_net(next_states)              # [B, n_actions]\n",
    "        q_next_max = q_next_all.max(dim=1, keepdim=True).values  # [B, 1]\n",
    "        q_target = rewards + (1.0 - dones) * GAMMA * q_next_max  # [B, 1]\n",
    "\n",
    "    # Pérdida MSE\n",
    "    loss = criterion(q_pred, q_target)\n",
    "\n",
    "    # Backprop\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # por si nos sirve, este es un clipping de gradiente para estabilidad, si no lo podemos borrar, x no importa\n",
    "    torch.nn.utils.clip_grad_norm_(online_net.parameters(), max_norm=10.0)\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
